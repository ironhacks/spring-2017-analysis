---
title: "Exploratory Factor Analysis"
output:
  html_notebook: default
  pdf_document: default
---

### （1）EFA with "pass class" survey item removed 

Participating in the hack doesn't affect the grades in the class. 

### (a) Notes on Total Variance Explained and Communalities

> While performing EFA using Principal Axis Factoring with Promax rotation, Osborne,
Costello, & Kellow (2008) suggests the communalities above 0.4 is acceptable

> Dawson (2016) suggests that the total value of Total Variance Explained should be between 50% and 90% for factor analysis. 

* Dawson, J. (2016). Analysing quantitative survey data for business and management students. Sage.

* Costello, AB & Osborne, Jason. (2005). Best Practices in Exploratory Factor Analysis: Four Recommendations for Getting the Most From Your Analysis. Practical Assessment, Research & Evaluation. 10. 1-9. 

### (b) Notes on Rotation Types

> "The next decision is rotation method. The goal of rotation is to simplify and clarify the data structure., there area variety of choices. Varimax rotation is by far themost common choice. Varimax, quartimax, and equamax are commonly available orthogonal methods of rotation; direct oblimin, quartimin, and promax are oblique. Orthogonal rotations produce factors that are
uncorrelated; oblique methods allow the factors to correlate. Conventional wisdom advises researchers to use orthogonal rotation because it produces moreeasily interpretable results, but this is a flawed argument. **In the social sciences we generally expect some correlation among factors, since behavior is rarely partitioned into neatly packaged units that function independently of one another. Therefore using orthogonal rotation results in a loss of valuable information if the factors are correlated, and oblique rotation should theoretically render a more accurate, and perhaps more reproducible, solution.** If the factors are truly uncorrelated, orthogonal and oblique
rotation produce nearly identical results."

* Costello, AB & Osborne, Jason. (2005). Best Practices in Exploratory Factor Analysis: Four Recommendations for Getting the Most From Your Analysis. Practical Assessment, Research & Evaluation. 10. 1-9. 


```{r}
# Loading required package: psych
require(psych)

# Loading required package: GPArotation
require(GPArotation)

```

```{r}
# Read in all survey items from raw Qualtrics data
survey <- read.table("/Users/jialincheoh/Desktop/survey_final_read.csv", header=TRUE, sep = ",")
survey
```


```{r}
# Rename all columns for motivation construct 
names(survey)[1:13] <- c("monetary_compensation", "excellent_programmer", "internship", 
                         "introduce_ideas", "get_in_touch", "known_creativity", "improve_skills", 
                         "test_capability", "enjoy_problems", "keep_up", "dissatisfaction", "curious", "pass_class")
survey

```
```{r}
# Extract all 13 survey items with motivation construct
motivation <- survey[c(1:13)]
```

### Summary of Data

```{r}
describe(motivation)
```

### Interpretation: 

> Response distributions for most items are approximately normal; the "improve_skills", "test_capability", "keep_up" items have excessive skewness, and the "improve_skills" item also has high kurtosis. 

> Looking forward, we might expect some bias (inaccuracy) in the factor loading estimates for the "improve_skills" item, particularly, but the maximum likelihood estimation that will be used for FA has some robustness to non-normality.

```{r}
correl = cor(motivation, use = 'pairwise.complete.obs')
symnum(correl)
correl
```

## Interpretation: 

> We want to make sure nothing on the off-diagonal is 1. 

```{r}
# Conducting Bartlett test
cortest.bartlett(correl, n=nrow(motivation))
```

## Interpretation: 

> Based on the p-value, it's a significant effect, which means the correlations are large enough for us to continue. 


```{r}
# KMO test
KMO(correl)
```

### Interpretation: 

> Comment: KMO low for "pass_class" survey item
  
> MSA The overall Measure of Sampling Adequacy

> MSAi The measure of sampling adequacy for each item item

> The Kaiser-Meyer-Olkin Measure of Sampling Adequacy is a statistic that indicates the proportion of variance in your variables that might be caused by underlying factors. High values (close to 1.0) generally indicate that a factor analysis may be useful for the data. If the value is less than 0.50, the results of the factor analysis probably won’t be very useful. Previous studies have suggested that KMO greater than 0.5 can be used for factor analysis, while KMO greater than 0.8 is very suitable for factor analysis. 

> Both the p.value attribute of cortest.bartlett()’s output is very much lower than 0.05 and the MSA attribute of KMO()’s output, 0.91, is close to 1, which means that they both recommend that EFA.

* Li, N., Huang, J., & Feng, Y. (2020). Construction and confirmatory factor analysis of the core cognitive ability index system of ship C2 system operators. PloS one, 15(8), e0237339. https://doi.org/10.1371/journal.pone.0237339

* Kaiser, H. F. (1974). An index of factorial simplicity. Psychometrika, 39(1), 31-36.


```{r}
hist(motivation$improve_skills)
```


```{r}
lowerCor(motivation)
```

### Interpretation: 

> Pearson correlations indicate the items have non-trivial pairwise interrelationships. Particularly, none of the item pairs have near-zero or very small correlations, except for the "pass_class" item. 

```{r}
library(correlation)

correlation::correlation(motivation,
                         include_factors = TRUE, method = "auto"
)

```

### Interpretation: 

> All the correlations are significant. 


```{r}
all_motivation <- fa.parallel(motivation, fm='ml', fa='fa', main="Parallel Analysis Scree Plots", n.iter=100, error.bars=T, sim=F)
all_motivation

```

## Interpretation: 

> Parallel analysis suggests that the number of factors =  3

```{r}
all_motivation <- fa(r=motivation, nfactors=2, rotate='promax', fm='ml')
all_motivation
```
## Interpretation: 

> ML1 - "improve_skills", "test_capability", "enjoy_problems", "keep_up" [ intrinsic ]

> ML2 = "excellent_programmer", "introduce_ideas", "get_in_touch", "known_creativity", "dissatisfaction" [ extrinsic ]



```{r}
all_motivation <- fa(r=motivation, nfactors=3, rotate='promax', fm='ml')
all_motivation
```

## Interpretation: 

> ML3 - "improve_skills", "test_capability", "enjoy_problems", "keep_up" [ intrinsic motivation ]

> ML1 - "known_creativity", "excellent_programmer" [ recognition ]

> ML2 - "introduce_ideas", "dissatisfaction" [ misc extrinc motivation ]


```{r}
motivation_9_items <- motivation[ , -c(1,3, 12, 13)]
motivation_9_items
```

```{r}
correl = cor(motivation_9_items, use = 'pairwise.complete.obs')
symnum(correl)
correl
```

```{r}
# Conducting Bartlett test
cortest.bartlett(correl, n=nrow(motivation_9_items))
```

## Interpretation: 

> p-value is significant which means that the correlation is large enough for us to continue. 


```{r}
KMO(motivation_9_items)
```



```{r}
fa.parallel(motivation_9_items, fm='ml', fa='fa', main="Parallel Analysis Scree Plots", n.iter=100, error.bars=T, sim=F)

```

## Interpretation: 

> Parallel analysis suggests that the number of factors =  2 


```{r}
# remove survey item 1, 3, 12, 13
# remove "monetary_compensation", "internship", "curious" and "pass_class" 

fa(r=motivation_9_items, nfactors=2, rotate='promax', fm='ml')
```
### Interpretation: 

> Items loaded on ML1 - "excellent_programmer", "introduce_ideas", "get_in_touch", "known_creativity" and "dissatisfaction". 

* "excellent_programmer", "introduce_ideas", "get_in_touch" and "known_creativity" are under "SHOW IDEAS" category in Fuller paper. "dissatisfaction" is under the category of "DISSATISFACTION" in Fuller paper. Both "SHOW IDEAS" and "DISSATISFACTION" lie on the "EXTRINSIC" side of motivations in Fuller paper. 

> Items loaded on ML2 - "improve_skills", "test_capability", "enjoy_problems", "keep_up"

* "improve_skills" and "test_capability"are under the category of "GAIN KNOWLEDGE" in Fuller paper. "enjoy_problems" and "keep_up" are under the category of "INTRINSIC INNOVATION INTEREST" in Fuller paper. Both "GAIN KNOWLEDGE" and "INTRINSIC INNOVATION INTEREST" lie on the "INTRINSIC" side of motivations in Fuller paper. 


> Tucker Lewis Index is a model fit statistic. Rule-of-thumb is that models with TLI > .90 fit the data well (Little, 2013, Longitudinal structural equation modeling)

* Comment: 0.924 ( Great )

> Chi-square test of model fit (below) tests the null hypothesis that “The model fits the data.” Want prob [p-value] > .05 to suggest that model fits data well. With “large” samples (say, greater than 400-500), minor misfit between each person’s data and the model may accumulate to produce a significant chi-square value. 

* Comment: 3e-04 ( Not Pass ) - not a big issue as reason above

> Root mean square error of approximation (RMSEA) is another model fit statistic. Rule-of-thumb is that models with RMSEA ≤ .05 fit the data well, ≤ .08 fit the data acceptably, and .08 - .10 is marginally acceptable fit (Little, 2013).

* Comment: 0.096 ( marginally acceptable fit )



```{r}
## get cfi of the 9 loaded survey items

finalmodel = fa(r=motivation[ , -c(1,3,12,13)], nfactors=2, rotate='promax', fm='ml')
1 - ((finalmodel$STATISTIC-finalmodel$dof)/
       (finalmodel$null.chisq-finalmodel$null.dof))
```

# Interpretation: 

> Very high CFI value of 0.96, larger than threhold of 0.95 


```{r}
## reliability for the 9 survey items. 

factor1 = c(7, 8, 9, 10)
factor2 = c(2, 4, 5, 6, 11)

psych::alpha(motivation[ , factor1])
psych::alpha(motivation[ , factor2])

```

# Interpretation: 

> raw_alpha for factor 1 is 0.86 which is good 

> raw_alpha for factor 2 is 0.84 which is good


```{r}
## create average factor scores for each person

motivation$f1 = apply(motivation[ , factor1], 1, sum)
motivation$f2 = apply(motivation[ , factor2], 1, sum)
summary(motivation)
```

```{r}
View(motivation)
```

## Validation

> Subsample for Bogota 0

```{r}

# subset of bogota 0
survey_bogota_0 <- subset(survey, hack_id == 0)
survey_bogota_0
```

```{r}
# bogota 0 all 13 motivation constructs

motivation_bogota_0 <- survey_bogota_0[c(1:13)]
motivation_bogota_0
```



```{r}
describe(motivation_bogota_0)
```

```{r}
KMO(motivation_bogota_0)
```

```{r}
lowerCor(motivation_bogota_0)
```
```{r}
fa.parallel(motivation_bogota_0[ , -c(1,3,12,13)], fm='ml', fa='fa', main="Parallel Analysis Scree Plots", n.iter=100, error.bars=T, sim=F)
```
 
```{r}
fa(r=motivation_bogota_0[ , -c(1,3,12,13)], nfactors=2, rotate='promax', fm='ml')
```

## Interpretation: 

> Cumulative variance now at 55%. "get_in_touch" moved to intrinsic category

> Items loaded on ML1 - "excellent_programmer", "introduce_ideas", "known_creativity", and "dissatisfaction". 

> Items loaded on ML2 - "improve_skills", "test_capability", "enjoy_problems", "get_in_touch"


```{r}
# subset of bogota 0
survey_bogota_1 <- subset(survey, hack_id == 1)
survey_bogota_1
```


```{r}
motivation_bogota_1 <- survey_bogota_1[c(1:13)]
motivation_bogota_1
```

```{r}
KMO(motivation_bogota_1)
```


```{r}
lowerCor(motivation_bogota_1)
```


```{r}
fa.parallel(motivation_bogota_1[ , -c(1,3,12,13)], fm='ml', fa='fa', main="Parallel Analysis Scree Plots", n.iter=100, error.bars=T, sim=F)
```

```{r}
fa(r=motivation_bogota_1[ , -c(1,3,12,13)], nfactors=2, rotate='oblimin', fm='ml')
```

> Cumulative variance now at 55%. enjoy_problems" moved to "extrinsic" category. 

> Items loaded on ML1 - "excellent_programmer", "introduce_ideas", "get_in_touch", "known_creativity",  "dissatisfaction", "enjoy_problems"

> Items loaded on ML2 - "improve_skills", "keep_up", "test_capability"


```{r}
# subset of bogota 2
survey_bogota_2 <- subset(survey, hack_id == 2)
survey_bogota_2
```

```{r}
motivation_bogota_2 <- survey_bogota_2[c(1:13)]
motivation_bogota_2
```

```{r}
describe(motivation_bogota_2)
```

```{r}
KMO(motivation_bogota_2)
```

```{r}
lowerCor(motivation_bogota_2)
```
```{r}
fa.parallel(motivation_bogota_2[ , -c(1,3,12,13)], fm='ml', fa='fa', main="Parallel Analysis Scree Plots", n.iter=100, error.bars=T, sim=F)
```

```{r}
fa(r=motivation_bogota_2[ , -c(1,3,12,13)], nfactors=2, rotate='oblimin', fm='ml')
```
> Cumulative variance now at 56%. "excellent_programmer moved to "intrinsic" category. 

> Items loaded on ML1 - "introduce_ideas", "get_in_touch", "known_creativity",  "dissatisfaction"

> Items loaded on ML2 - "improve_skills", "keep_up", "test_capability", "enjoy_problems", "excellent_programmer"

```{r}
# subset of bogota 3
survey_bogota_3 <- subset(survey, hack_id == 3)
survey_bogota_3
```

```{r}
motivation_bogota_3 <- survey_bogota_3[c(1:13)]
motivation_bogota_3
```


```{r}
describe(motivation_bogota_3)
```

```{r}
KMO(motivation_bogota_3)
```

```{r}
fa.parallel(motivation_bogota_3[ , -c(1,3,12,13)], fm='ml', fa='fa', main="Parallel Analysis Scree Plots", n.iter=100, error.bars=T, sim=F)
```

```{r}
fa(r=motivation_bogota_3[ , -c(1,3,12,13)], nfactors=2, rotate='promax', fm='ml')
```

### Interpretations: 

> Only one item loads on the second factor, best to retain only one factor. 


## Confirmatory Factor Analaysis for Subsample


```{r}
# Then we define the model by specifying the relationship between items and factors:

path <-
'f1 =~ excellent_programmer + introduce_ideas + get_in_touch + known_creativity + dissatisfaction
f2 =~ improve_skills + test_capability + enjoy_problems + keep_up'
```

```{r}
library(lavaan)
model <- cfa(path, data= motivation_bogota_0)
summary(model, fit.measures=TRUE)
```

```{r}
library(lavaan)
model <- cfa(path, data= motivation_bogota_1)
summary(model, fit.measures=TRUE)
```


```{r}
library(lavaan)
model <- cfa(path, data= motivation_bogota_2)
summary(model, fit.measures=TRUE)
```

```{r}
library(lavaan)
model <- cfa(path, data= motivation_bogota_3)
summary(model, fit.measures=TRUE)
```


```{r}
allData <- cbind(motivation, id = survey$hack_id)
allData
```


```{r}
# subset of dataframe for anova factor 1
anova_1 <- allData[c(14, 16)]
anova_1
```


```{r}

# Performing anova on factor 1
ANOVA_RESULTS_1 <- aov(f1 ~ id, data=anova_1)
summary(ANOVA_RESULTS_1)
```

```{r}
# subset of dataframe for anova factor 2
anova_2 <- allData[c(15, 16)]
anova_2
```

```{r}

# Performing anova on factor 2
ANOVA_RESULTS_2 <- aov(f2 ~ id, data=anova_2)
summary(ANOVA_RESULTS_2)

```

```{r}

```

## Interpretation: 

> Factor 1 and Factor 2 are both not significant across the groups. 

```{r}
# Maximum Likelihood Factor Analysis
# entering raw data and extracting 3 factors,
# with varimax rotation
fit <- factanal(motivation[1:13], 2, rotation="varimax", scores="Bartlett")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2
load <- fit$loadings[,1:2]
plot(load,type="n") # set up plot
text(load,labels=names(motivation)) # add variable names
```

```{r}
fit_matrix <- fit$scores
fit_matrix
```

## PCs as Data Frame from Matrix 

```{r}
fit_df <- as.data.frame(fit_matrix)
fit_df
```

## Combine hack_id with Factor Scores 

```{r}
overall_fit <- cbind(fit_df, id = survey$hack_id)
overall_fit
```

## Combine Performance anmd Solution Groupings. 

```{r}
overall_fit$performance <- ifelse(overall_fit$id==1, 1,
        ifelse(overall_fit$id==2, 0,
        ifelse(overall_fit$id==0, 0,
        ifelse(overall_fit$id==3, 1, NA))))

overall_fit
```

```{r}
overall_fit$solution <- ifelse(overall_fit$id==1, 0,
        ifelse(overall_fit$id==2, 1,
        ifelse(overall_fit$id==0, 0,
        ifelse(overall_fit$id==3, 1, NA))))

overall_fit
```


```{r}
overall_fit_abs <- abs(overall_fit)
overall_fit_abs
```


```{r}
allData <- cbind(survey, overall_fit_abs)
allData
```


```{r}
survey_bogota_0 <- subset(allData, hack_id == 0)
survey_bogota_0
```

```{r}
describe(survey_bogota_0$Factor1)
```

```{r}
plot(survey_bogota_0$Factor1)
```


## Descriptive Analysis for both factor 1 and factor 2

```{r}
describe(overall_fit_abs$Factor1)
```

```{r}
plot(overall_fit_abs$Factor1)
```
```{r}
describe(overall_fit_abs$Factor2)
```

```{r}
plot(overall_fit_abs$Factor2)
```

## ANOVA across all 4 groups for Factor 1

```{r}
anova_1 <- aov(Factor1 ~ id, data=overall_fit_abs)
summary(anova_1)
```

## Interpretation: 

> No significance across all the 4 groups. 

## ANOVA across all 4 groups for Factor 2

```{r}
anova_2 <- aov(Factor2 ~ id, data=overall_fit_abs)
summary(anova_2)
```

## Interpretation: 

> No significance across all the 4 groups. 


## Unpaired t-test for Factor 1 [ 2 groups ]

```{r}
res<-t.test(overall_fit_abs$performance, overall_fit_abs$Factor1 )
res
```

## Interpretation: 

> Difference is significant 

```{r}
res<-t.test(overall_fit_abs$solution, overall_fit_abs$Factor1 )
res
```

## Interpretation: 

> Difference is significant 

## Unpaired t-test for Factor 2 [ 2 groups ]

```{r}
res<-t.test(overall_fit_abs$performance, overall_fit_abs$Factor2 )
res

```

## Interpretation: 

> Difference is significant 

```{r}
res<-t.test(overall_fit_abs$solution, overall_fit_abs$Factor2 )
res
```

## Interpretation: 

> Difference is significant 

```{r}
fa.res<-factanal(x=motivation[1:12], factors=3, rotation='promax')
print(fa.res, cut=0.5)
```
## Interpretation: 

The p-value for the chi-square test is 0.191 which is larger than .05. Therefore,we fail to reject the null hypothesis that the factor model have a good fit to the data.

```{r}
fa.res<-factanal(x=motivation[1:12], factors=3, rotation='promax', scores='Bartlett')
head(fa.res$scores)
```
```{r}
summary(lm(Factor2 ~ Factor1, data=as.data.frame(fa.res$scores)))
```
```{r}
https://online.stat.psu.edu/stat503/moti
```

