{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61356057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.api import add_constant\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "\n",
    "%store -r perform_dist_high_compare_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55eb830e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Student</th>\n",
       "      <th>group</th>\n",
       "      <th>abs_perform_diff_best</th>\n",
       "      <th>phase</th>\n",
       "      <th>Q7_Q7_1</th>\n",
       "      <th>Q7_Q7_2</th>\n",
       "      <th>Q8_Q8_1</th>\n",
       "      <th>Q10</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bdvegat</td>\n",
       "      <td>2</td>\n",
       "      <td>260.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Phoenixest</td>\n",
       "      <td>2</td>\n",
       "      <td>185.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HashNick</td>\n",
       "      <td>2</td>\n",
       "      <td>260.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.028302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ccvacad</td>\n",
       "      <td>2</td>\n",
       "      <td>260.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>joaortizro</td>\n",
       "      <td>2</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.041420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>lsfinite</td>\n",
       "      <td>2</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.008392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>xdanielsb</td>\n",
       "      <td>2</td>\n",
       "      <td>283.33</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.040128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>164</td>\n",
       "      <td>CSebasGomez</td>\n",
       "      <td>2</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>165</td>\n",
       "      <td>jhcardenasa</td>\n",
       "      <td>2</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>167</td>\n",
       "      <td>jscontrerasp</td>\n",
       "      <td>2</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       Student  group  abs_perform_diff_best  phase  Q7_Q7_1  \\\n",
       "1             1       bdvegat      2                 260.00      1      1.0   \n",
       "2             2    Phoenixest      2                 185.83      1      0.0   \n",
       "3             3      HashNick      2                 260.00      1      1.0   \n",
       "4             4       ccvacad      2                 260.00      1      1.0   \n",
       "5             5    joaortizro      2                 152.50      1      0.0   \n",
       "..          ...           ...    ...                    ...    ...      ...   \n",
       "162         162      lsfinite      2                 358.33      4      1.0   \n",
       "163         163     xdanielsb      2                 283.33      4      3.0   \n",
       "164         164   CSebasGomez      2                 358.33      4      3.0   \n",
       "165         165   jhcardenasa      2                 358.33      4      4.0   \n",
       "167         167  jscontrerasp      2                 358.33      4      1.0   \n",
       "\n",
       "     Q7_Q7_2  Q8_Q8_1  Q10  similarity  \n",
       "1        2.0      4.0  1.0    0.018072  \n",
       "2        1.0      3.0  1.0    0.017964  \n",
       "3        3.0      5.0  3.0    0.028302  \n",
       "4        0.0      4.0  2.0    0.000000  \n",
       "5        2.0      2.0  1.0    0.041420  \n",
       "..       ...      ...  ...         ...  \n",
       "162      1.0      6.0  3.0    0.008392  \n",
       "163      3.0      3.0  2.0    0.040128  \n",
       "164      3.0      5.0  2.0    0.000000  \n",
       "165      4.0      5.0  3.0    0.000000  \n",
       "167      1.0      4.0  0.0    0.054455  \n",
       "\n",
       "[156 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('perform_dist_high_compare_b2.csv', header = 0)\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d9ca75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              OLS Regression Results                             \n",
      "=================================================================================\n",
      "Dep. Variable:     abs_perform_diff_best   R-squared:                       0.002\n",
      "Model:                               OLS   Adj. R-squared:                 -0.005\n",
      "Method:                    Least Squares   F-statistic:                    0.2540\n",
      "Date:                   Sat, 24 Sep 2022   Prob (F-statistic):              0.615\n",
      "Time:                           23:26:59   Log-Likelihood:                -915.75\n",
      "No. Observations:                    156   AIC:                             1836.\n",
      "Df Residuals:                        154   BIC:                             1842.\n",
      "Df Model:                              1                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    199.5418     16.779     11.893      0.000     166.396     232.688\n",
      "Q10           -4.3559      8.642     -0.504      0.615     -21.429      12.717\n",
      "==============================================================================\n",
      "Omnibus:                        5.239   Durbin-Watson:                   1.146\n",
      "Prob(Omnibus):                  0.073   Jarque-Bera (JB):                3.041\n",
      "Skew:                          -0.115   Prob(JB):                        0.219\n",
      "Kurtosis:                       2.356   Cond. No.                         5.79\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                          IV2SLS Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          ln_similarity   R-squared:                     -63.421\n",
      "Model:                         IV2SLS   Adj. R-squared:                -63.839\n",
      "Method:                     Two Stage   F-statistic:                    0.2398\n",
      "                        Least Squares   Prob (F-statistic):              0.625\n",
      "Date:                Sat, 24 Sep 2022                                         \n",
      "Time:                        23:26:59                                         \n",
      "No. Observations:                 156                                         \n",
      "Df Residuals:                     154                                         \n",
      "Df Model:                           1                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "const                    -0.5449      1.202     -0.453      0.651      -2.920       1.830\n",
      "abs_perform_diff_best     0.0031      0.006      0.490      0.625      -0.009       0.015\n",
      "==============================================================================\n",
      "Omnibus:                        5.039   Durbin-Watson:                   1.169\n",
      "Prob(Omnibus):                  0.080   Jarque-Bera (JB):                2.920\n",
      "Skew:                           0.100   Prob(JB):                        0.232\n",
      "Kurtosis:                       2.360   Cond. No.                         515.\n",
      "==============================================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          ln_similarity   R-squared:                       0.026\n",
      "Model:                            OLS   Adj. R-squared:                  0.020\n",
      "Method:                 Least Squares   F-statistic:                     4.089\n",
      "Date:                Sat, 24 Sep 2022   Prob (F-statistic):             0.0449\n",
      "Time:                        23:26:59   Log-Likelihood:                 309.43\n",
      "No. Observations:                 156   AIC:                            -614.9\n",
      "Df Residuals:                     154   BIC:                            -608.8\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                 0.0558      0.007      8.491      0.000       0.043       0.069\n",
      "abs_perform_diff_best -6.322e-05   3.13e-05     -2.022      0.045      -0.000   -1.46e-06\n",
      "==============================================================================\n",
      "Omnibus:                        9.813   Durbin-Watson:                   1.905\n",
      "Prob(Omnibus):                  0.007   Jarque-Bera (JB):               10.593\n",
      "Skew:                           0.631   Prob(JB):                      0.00501\n",
      "Kurtosis:                       2.808   Cond. No.                         515.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Let's confirm that aspiration satisfy the relevance condition for performance distance to the best \n",
    "reg_expr = 'abs_perform_diff_best ~ Q10'\n",
    "\n",
    "# Build and train an OLS model that regresses performance distance to the best on aspiration and verify\n",
    "# using the F-test that coefficients of aspiration is significant \n",
    "olsr_model = smf.ols(formula=reg_expr, data=df)\n",
    "olsr_model_results = olsr_model.fit()\n",
    "print(olsr_model_results.summary())\n",
    "\n",
    "df['ln_similarity'] = np.log(df['similarity'] + 1)\n",
    "\n",
    "# Build out the exog matrix. Statsmodels requires this matrix to contain all the endogenous and\n",
    "# exogenous variables, plus the constant.\n",
    "exog = df[['abs_perform_diff_best']]\n",
    "exog = add_constant(exog)\n",
    "\n",
    "# Build out the instruments matrix. Statsmodels requires this matrix to contain not only all the\n",
    "# instruments but also the variables in exog that will NOT be instrumented\n",
    "instruments = df[['Q10']]\n",
    "instruments = add_constant(instruments)\n",
    "\n",
    "#Build and train the IV2SLS model\n",
    "iv2sls_model = IV2SLS(endog=df['ln_similarity'], exog=exog, instrument=instruments)\n",
    "iv2sls_model_results = iv2sls_model.fit()\n",
    "\n",
    "#Print the training summary\n",
    "print(iv2sls_model_results.summary())\n",
    "\n",
    "#Compare the performance of 2SLS with OLS of ln(wage) on performance distance to the best\n",
    "reg_expr = 'ln_similarity ~ abs_perform_diff_best'\n",
    "olsr_model = smf.ols(formula=reg_expr, data=df)\n",
    "olsr_model_results = olsr_model.fit()\n",
    "print(olsr_model_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de43d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'solution' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "solution = df\n",
    "%store solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e34c62",
   "metadata": {},
   "source": [
    "## Phase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aca694fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Student</th>\n",
       "      <th>group</th>\n",
       "      <th>abs_perform_diff_best</th>\n",
       "      <th>phase</th>\n",
       "      <th>Q7_Q7_1</th>\n",
       "      <th>Q7_Q7_2</th>\n",
       "      <th>Q8_Q8_1</th>\n",
       "      <th>Q10</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>127</td>\n",
       "      <td>bdvegat</td>\n",
       "      <td>2</td>\n",
       "      <td>263.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.031579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>128</td>\n",
       "      <td>Phoenixest</td>\n",
       "      <td>2</td>\n",
       "      <td>263.33</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>129</td>\n",
       "      <td>HashNick</td>\n",
       "      <td>2</td>\n",
       "      <td>226.66</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.056277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>130</td>\n",
       "      <td>ccvacad</td>\n",
       "      <td>2</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.066406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>131</td>\n",
       "      <td>joaortizro</td>\n",
       "      <td>2</td>\n",
       "      <td>78.33</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>132</td>\n",
       "      <td>juasmartinezbel</td>\n",
       "      <td>2</td>\n",
       "      <td>37.50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.080780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>133</td>\n",
       "      <td>mdbelloc</td>\n",
       "      <td>2</td>\n",
       "      <td>229.16</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.053097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>134</td>\n",
       "      <td>diegocruz10</td>\n",
       "      <td>2</td>\n",
       "      <td>251.66</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.056338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>crarojasca</td>\n",
       "      <td>2</td>\n",
       "      <td>313.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>136</td>\n",
       "      <td>sagilm</td>\n",
       "      <td>2</td>\n",
       "      <td>208.33</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.047393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>137</td>\n",
       "      <td>idrojasf</td>\n",
       "      <td>2</td>\n",
       "      <td>205.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.014286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>138</td>\n",
       "      <td>raulramirezp</td>\n",
       "      <td>2</td>\n",
       "      <td>210.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.076577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>140</td>\n",
       "      <td>jhriverasa</td>\n",
       "      <td>2</td>\n",
       "      <td>25.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.054475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>141</td>\n",
       "      <td>jhonsd1996</td>\n",
       "      <td>2</td>\n",
       "      <td>200.83</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.016043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>142</td>\n",
       "      <td>JuanitoAli</td>\n",
       "      <td>2</td>\n",
       "      <td>33.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.013460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>143</td>\n",
       "      <td>Davsatu313</td>\n",
       "      <td>2</td>\n",
       "      <td>57.50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.120172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>144</td>\n",
       "      <td>cesarochoa2006</td>\n",
       "      <td>2</td>\n",
       "      <td>74.37</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>145</td>\n",
       "      <td>jjmuesesq</td>\n",
       "      <td>2</td>\n",
       "      <td>250.83</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>146</td>\n",
       "      <td>jucjimenezmo</td>\n",
       "      <td>2</td>\n",
       "      <td>155.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.064103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>kancas</td>\n",
       "      <td>2</td>\n",
       "      <td>151.66</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.060748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>148</td>\n",
       "      <td>Gantiva</td>\n",
       "      <td>2</td>\n",
       "      <td>120.83</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.077626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>149</td>\n",
       "      <td>Dandarprox</td>\n",
       "      <td>2</td>\n",
       "      <td>145.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.062130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>150</td>\n",
       "      <td>debeltranc</td>\n",
       "      <td>2</td>\n",
       "      <td>258.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>151</td>\n",
       "      <td>ykchautai</td>\n",
       "      <td>2</td>\n",
       "      <td>159.16</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.046083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>152</td>\n",
       "      <td>ligibrial</td>\n",
       "      <td>2</td>\n",
       "      <td>250.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.011617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>153</td>\n",
       "      <td>davidrh195</td>\n",
       "      <td>2</td>\n",
       "      <td>250.83</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>154</td>\n",
       "      <td>JulianaNino</td>\n",
       "      <td>2</td>\n",
       "      <td>234.16</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.044776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155</td>\n",
       "      <td>luegilca</td>\n",
       "      <td>2</td>\n",
       "      <td>241.66</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.038095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156</td>\n",
       "      <td>Yelis3</td>\n",
       "      <td>2</td>\n",
       "      <td>124.16</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.004959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>157</td>\n",
       "      <td>begarzonf</td>\n",
       "      <td>2</td>\n",
       "      <td>143.33</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.024917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>158</td>\n",
       "      <td>kmendezp96</td>\n",
       "      <td>2</td>\n",
       "      <td>151.66</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>leguzman</td>\n",
       "      <td>2</td>\n",
       "      <td>68.33</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>luealfonsoru</td>\n",
       "      <td>2</td>\n",
       "      <td>220.83</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.054688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>oserasoa</td>\n",
       "      <td>2</td>\n",
       "      <td>242.50</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>lsfinite</td>\n",
       "      <td>2</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.008392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>xdanielsb</td>\n",
       "      <td>2</td>\n",
       "      <td>283.33</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.040128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>164</td>\n",
       "      <td>CSebasGomez</td>\n",
       "      <td>2</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>165</td>\n",
       "      <td>jhcardenasa</td>\n",
       "      <td>2</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>167</td>\n",
       "      <td>jscontrerasp</td>\n",
       "      <td>2</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0          Student  group  abs_perform_diff_best  phase  \\\n",
       "127         127          bdvegat      2                 263.33      4   \n",
       "128         128       Phoenixest      2                 263.33      4   \n",
       "129         129         HashNick      2                 226.66      4   \n",
       "130         130          ccvacad      2                 358.33      4   \n",
       "131         131       joaortizro      2                  78.33      4   \n",
       "132         132  juasmartinezbel      2                  37.50      4   \n",
       "133         133         mdbelloc      2                 229.16      4   \n",
       "134         134      diegocruz10      2                 251.66      4   \n",
       "135         135       crarojasca      2                 313.33      4   \n",
       "136         136           sagilm      2                 208.33      4   \n",
       "137         137         idrojasf      2                 205.00      4   \n",
       "138         138     raulramirezp      2                 210.00      4   \n",
       "140         140       jhriverasa      2                  25.00      4   \n",
       "141         141       jhonsd1996      2                 200.83      4   \n",
       "142         142       JuanitoAli      2                  33.33      4   \n",
       "143         143       Davsatu313      2                  57.50      4   \n",
       "144         144   cesarochoa2006      2                  74.37      4   \n",
       "145         145        jjmuesesq      2                 250.83      4   \n",
       "146         146     jucjimenezmo      2                 155.00      4   \n",
       "147         147           kancas      2                 151.66      4   \n",
       "148         148          Gantiva      2                 120.83      4   \n",
       "149         149       Dandarprox      2                 145.00      4   \n",
       "150         150       debeltranc      2                 258.33      4   \n",
       "151         151        ykchautai      2                 159.16      4   \n",
       "152         152        ligibrial      2                 250.00      4   \n",
       "153         153       davidrh195      2                 250.83      4   \n",
       "154         154      JulianaNino      2                 234.16      4   \n",
       "155         155         luegilca      2                 241.66      4   \n",
       "156         156           Yelis3      2                 124.16      4   \n",
       "157         157        begarzonf      2                 143.33      4   \n",
       "158         158       kmendezp96      2                 151.66      4   \n",
       "159         159         leguzman      2                  68.33      4   \n",
       "160         160     luealfonsoru      2                 220.83      4   \n",
       "161         161         oserasoa      2                 242.50      4   \n",
       "162         162         lsfinite      2                 358.33      4   \n",
       "163         163        xdanielsb      2                 283.33      4   \n",
       "164         164      CSebasGomez      2                 358.33      4   \n",
       "165         165      jhcardenasa      2                 358.33      4   \n",
       "167         167     jscontrerasp      2                 358.33      4   \n",
       "\n",
       "     Q7_Q7_1  Q7_Q7_2  Q8_Q8_1  Q10  similarity  \n",
       "127      1.0      2.0      4.0  1.0    0.031579  \n",
       "128      0.0      1.0      3.0  1.0    0.016043  \n",
       "129      1.0      3.0      5.0  3.0    0.056277  \n",
       "130      1.0      0.0      4.0  2.0    0.066406  \n",
       "131      0.0      2.0      2.0  1.0    0.040816  \n",
       "132      0.0      5.0      5.0  2.0    0.080780  \n",
       "133      1.0      3.0      2.0  2.0    0.053097  \n",
       "134      3.0      3.0      3.0  1.0    0.056338  \n",
       "135      1.0      3.0      5.0  2.0    0.000000  \n",
       "136      0.0      0.0      3.0  1.0    0.047393  \n",
       "137      1.0      1.0      4.0  4.0    0.014286  \n",
       "138      1.0      2.0      5.0  2.0    0.076577  \n",
       "140      1.0      3.0      2.0  2.0    0.054475  \n",
       "141      3.0      4.0      4.0  2.0    0.016043  \n",
       "142      1.0      1.0      5.0  2.0    0.013460  \n",
       "143      0.0      3.0      5.0  2.0    0.120172  \n",
       "144      0.0      1.0      6.0  0.0    0.060000  \n",
       "145      0.0      3.0      2.0  2.0    0.039216  \n",
       "146      0.0      0.0      3.0  2.0    0.064103  \n",
       "147      0.0      0.0      5.0  2.0    0.060748  \n",
       "148      2.0      4.0      3.0  2.0    0.077626  \n",
       "149      0.0      1.0      4.0  2.0    0.062130  \n",
       "150      1.0      4.0      4.0  1.0    0.000000  \n",
       "151      4.0      4.0      4.0  1.0    0.046083  \n",
       "152      1.0      5.0      3.0  2.0    0.011617  \n",
       "153      3.0      3.0      5.0  1.0    0.014851  \n",
       "154      1.0      1.0      3.0  1.0    0.044776  \n",
       "155      0.0      1.0      6.0  2.0    0.038095  \n",
       "156      1.0      4.0      6.0  3.0    0.004959  \n",
       "157      2.0      3.0      4.0  2.0    0.024917  \n",
       "158      1.0      5.0      6.0  1.0    0.014877  \n",
       "159      0.0      2.0      5.0  2.0    0.090909  \n",
       "160      3.0      4.0      4.0  2.0    0.054688  \n",
       "161      4.0      3.0      3.0  1.0    0.071749  \n",
       "162      1.0      1.0      6.0  3.0    0.008392  \n",
       "163      3.0      3.0      3.0  2.0    0.040128  \n",
       "164      3.0      3.0      5.0  2.0    0.000000  \n",
       "165      4.0      4.0      5.0  3.0    0.000000  \n",
       "167      1.0      1.0      4.0  0.0    0.054455  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('perform_dist_high_compare_b2.csv', header = 0)\n",
    "df = df.dropna()\n",
    "df = df[df['phase'] == 4]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45dcd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              OLS Regression Results                             \n",
      "=================================================================================\n",
      "Dep. Variable:     abs_perform_diff_best   R-squared:                       0.001\n",
      "Model:                               OLS   Adj. R-squared:                 -0.026\n",
      "Method:                    Least Squares   F-statistic:                   0.02584\n",
      "Date:                   Sat, 24 Sep 2022   Prob (F-statistic):              0.873\n",
      "Time:                           23:26:59   Log-Likelihood:                -232.99\n",
      "No. Observations:                     39   AIC:                             470.0\n",
      "Df Residuals:                         37   BIC:                             473.3\n",
      "Df Model:                              1                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    197.5375     37.976      5.202      0.000     120.591     274.484\n",
      "Q10            3.1442     19.561      0.161      0.873     -36.489      42.778\n",
      "==============================================================================\n",
      "Omnibus:                        1.437   Durbin-Watson:                   1.091\n",
      "Prob(Omnibus):                  0.487   Jarque-Bera (JB):                1.092\n",
      "Skew:                          -0.149   Prob(JB):                        0.579\n",
      "Kurtosis:                       2.236   Cond. No.                         5.79\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                          IV2SLS Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          ln_similarity   R-squared:                     -31.797\n",
      "Model:                         IV2SLS   Adj. R-squared:                -32.684\n",
      "Method:                     Two Stage   F-statistic:                   0.02924\n",
      "                        Least Squares   Prob (F-statistic):              0.865\n",
      "Date:                Sat, 24 Sep 2022                                         \n",
      "Time:                        23:26:59                                         \n",
      "No. Observations:                  39                                         \n",
      "Df Residuals:                      37                                         \n",
      "Df Model:                           1                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "const                     0.3962      2.080      0.190      0.850      -3.819       4.611\n",
      "abs_perform_diff_best    -0.0018      0.010     -0.171      0.865      -0.023       0.019\n",
      "==============================================================================\n",
      "Omnibus:                        0.699   Durbin-Watson:                   1.122\n",
      "Prob(Omnibus):                  0.705   Jarque-Bera (JB):                0.756\n",
      "Skew:                          -0.147   Prob(JB):                        0.685\n",
      "Kurtosis:                       2.384   Cond. No.                         529.\n",
      "==============================================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          ln_similarity   R-squared:                       0.192\n",
      "Model:                            OLS   Adj. R-squared:                  0.171\n",
      "Method:                 Least Squares   F-statistic:                     8.814\n",
      "Date:                Sat, 24 Sep 2022   Prob (F-statistic):            0.00522\n",
      "Time:                        23:26:59   Log-Likelihood:                 89.201\n",
      "No. Observations:                  39   AIC:                            -174.4\n",
      "Df Residuals:                      37   BIC:                            -171.1\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                 0.0661      0.010      6.944      0.000       0.047       0.085\n",
      "abs_perform_diff_best    -0.0001   4.25e-05     -2.969      0.005      -0.000      -4e-05\n",
      "==============================================================================\n",
      "Omnibus:                        0.938   Durbin-Watson:                   2.092\n",
      "Prob(Omnibus):                  0.626   Jarque-Bera (JB):                0.830\n",
      "Skew:                           0.055   Prob(JB):                        0.660\n",
      "Kurtosis:                       2.294   Cond. No.                         529.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Let's confirm that aspiration satisfy the relevance condition for performance distance to the best. There is a difference between aspiration and social aspiration. \n",
    "reg_expr = 'abs_perform_diff_best ~ Q10'\n",
    "\n",
    "# Build and train an OLS model that regresses performance distance to the best on aspiration and verify\n",
    "# using the F-test that coefficients of aspiration is significant \n",
    "olsr_model = smf.ols(formula=reg_expr, data=df)\n",
    "olsr_model_results = olsr_model.fit()\n",
    "print(olsr_model_results.summary())\n",
    "\n",
    "df['ln_similarity'] = np.log(df['similarity'] + 1)\n",
    "\n",
    "# Build out the exog matrix. Statsmodels requires this matrix to contain all the endogenous and\n",
    "# exogenous variables, plus the constant.\n",
    "exog = df[['abs_perform_diff_best']]\n",
    "exog = add_constant(exog)\n",
    "\n",
    "# Build out the instruments matrix. Statsmodels requires this matrix to contain not only all the\n",
    "# instruments but also the variables in exog that will NOT be instrumented\n",
    "instruments = df[['Q10']]\n",
    "instruments = add_constant(instruments)\n",
    "\n",
    "#Build and train the IV2SLS model\n",
    "iv2sls_model = IV2SLS(endog=df['ln_similarity'], exog=exog, instrument=instruments)\n",
    "iv2sls_model_results = iv2sls_model.fit()\n",
    "\n",
    "#Print the training summary\n",
    "print(iv2sls_model_results.summary())\n",
    "\n",
    "#Compare the performance of 2SLS with OLS of ln(wage) on performance distance to the best\n",
    "reg_expr = 'ln_similarity ~ abs_perform_diff_best'\n",
    "olsr_model = smf.ols(formula=reg_expr, data=df)\n",
    "olsr_model_results = olsr_model.fit()\n",
    "print(olsr_model_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816cff2",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0da5ae",
   "metadata": {},
   "source": [
    "![image1a](image1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf987c",
   "metadata": {},
   "source": [
    "The coefficients of aspiration (Q10) is significant at a p of < 0 as indicated by their p-values which are basically zero. Aspiration (Q10) clearly meet the **relevance condition** for instrumental variables of performance distance. \n",
    "\n",
    "Weâ€™ll now build a linear model for the similarity to the best equation and using statsmodels, weâ€™ll train the model using the two-stage least square estimator.\n",
    "\n",
    "Weâ€™ll start by building the design matrices. The dependent variable is ln(similarity). \n",
    "\n",
    "```\n",
    "ln_wage = np.log(df['similarity'])\n",
    "```\n",
    "\n",
    "Statsmodelâ€™s IV2SLS estimator is defined as follows:\n",
    "\n",
    "```\n",
    "\n",
    "statsmodels.sandbox.regression.gmm.IV2SLS(endog, exog, instrument=None)\n",
    "\n",
    "```\n",
    "Statsmodels needs the endog, exog and instrument matrices to be constructed in a specific way as follows:\n",
    "\n",
    "`endog` is an [n x 1] matrix containing the dependent variable. In our example, it is the log(similarity) variable.\n",
    "\n",
    "`exog` is an [n x (k+1)] size matrix that must contain all the endogenous and exogenous variables, plus the constant. In our example, apart from the constant, we do not have any exogenous variables defined in our wage equation. \n",
    "\n",
    "`instrument` is a matrix that contains the instrumental variables. Additionally, the Statsmodelsâ€™ IV2SLS estimator requires instrument to also contain all variables from the exog matrix that are not being instrumented. In our example, the instrumental variable is aspiration. The variables in exog that are not being instrumented is just the placeholder column for the intercept. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e56287",
   "metadata": {},
   "source": [
    "![image2a](image2a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2aa2cf",
   "metadata": {},
   "source": [
    "## Interpretation of results of the 2SLS model\n",
    "\n",
    "Since our primary interest is in estimating the effect of performance distance to the best on similarity to the best, weâ€™ll focus our attention on the coefficient estimate of the performance distance to the best. \n",
    "\n",
    "We see that the 2SLS model has estimated the coefficient of performance distance to the best as -7.251e-05 with a standard error of 0.000 and a 95% confidence interval of -0.000 to 0.000. The p-value of 0.084 suggests a significance at (1â€“0.702)100%=29.8%. Overall, and as expected for a 2SLS model, the model lacks precision.\n",
    "\n",
    "Note that dependent variable is log(similarity + 1). To calculate the rate of change of similarity to the best for each unit change of performance distance to the best, we must exponentiate the coefficient of performance distance to the best.\n",
    "\n",
    "1 - 10^(-7.251e-05) = 0.000167 implying that a unit decrease in performance distance to the best is estimated to yield an increase of 0.000167 in similarity to the best, and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5794b",
   "metadata": {},
   "source": [
    "## Comparison of the IV estimator with an OLS estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08baa040",
   "metadata": {},
   "source": [
    "Letâ€™s compare the performance of the 2SLS model with a straight-up OLS model that regresses log(similarity) on performance distance to the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf0fc0d",
   "metadata": {},
   "source": [
    "![image3a.png](image3a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f06eb8e",
   "metadata": {},
   "source": [
    "Weâ€™ll focus our attention on the estimated value of the coefficient of performance distance to the best. At -0.0002, it is a lot higher than the estimate reported by the 2SLS model.\n",
    "\n",
    "1 - 10^(-0.0002)=0.00046, implying a unit decrease in the performance distance to the best is estimated to translate into a 0.00046 increase in similarity to the best (vice versa). \n",
    "\n",
    "The higher estimate from OLS is expected due to the suspected endogeniety of performance distance to the best. In practice, depending on the situation we are modeling, we may want to accept the more conservative estimate of -7.251e-05 reported by the 2SLS model. However, (and against the 2SLS model), the coefficient estimate from the OLS model is highly significant with a p-value that is 0.000. Recollect that the estimate from the 2SLS model was significant at only a 29.8% confidence level.\n",
    "\n",
    "The coefficient estimate of performance distance to the best reported by the OLS model has pretty similar standard error which is zero as compared to that from the 2SLS model. \n",
    "\n",
    "For comparison, here are the coefficient estimates of performance distance to the best and corresponding 95% CIs from the two models:\n",
    "\n",
    "With the IV estimator, one trades precision of estimates for the removal of endogeneity and the consequent bias in the estimates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4b679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
