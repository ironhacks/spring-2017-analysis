{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61356057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.api import add_constant\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "\n",
    "%store -r perform_dist_high_compare_b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa281323",
   "metadata": {},
   "source": [
    "## Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55eb830e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Student</th>\n",
       "      <th>group</th>\n",
       "      <th>abs_perform_diff_best</th>\n",
       "      <th>phase</th>\n",
       "      <th>Q7_Q7_1</th>\n",
       "      <th>Q7_Q7_2</th>\n",
       "      <th>Q8_Q8_1</th>\n",
       "      <th>Q10</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lemartinp</td>\n",
       "      <td>3</td>\n",
       "      <td>270.84</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.146341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>juligarji</td>\n",
       "      <td>3</td>\n",
       "      <td>167.50</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.057325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>nfmorenog</td>\n",
       "      <td>3</td>\n",
       "      <td>225.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AFelipeGA</td>\n",
       "      <td>3</td>\n",
       "      <td>291.67</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.003841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mballeng91</td>\n",
       "      <td>3</td>\n",
       "      <td>329.17</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.004854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>NicolasPrr</td>\n",
       "      <td>3</td>\n",
       "      <td>237.50</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>jumcorredorro</td>\n",
       "      <td>3</td>\n",
       "      <td>53.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.013294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>feartheGru</td>\n",
       "      <td>3</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>Danielsv9207</td>\n",
       "      <td>3</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>JhonEmmanuelTorres</td>\n",
       "      <td>3</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0             Student  group  abs_perform_diff_best  phase  \\\n",
       "0             0           lemartinp      3                 270.84      1   \n",
       "1             1           juligarji      3                 167.50      1   \n",
       "2             2           nfmorenog      3                 225.00      1   \n",
       "3             3           AFelipeGA      3                 291.67      1   \n",
       "4             4          mballeng91      3                 329.17      1   \n",
       "..          ...                 ...    ...                    ...    ...   \n",
       "159         159          NicolasPrr      3                 237.50      4   \n",
       "160         160       jumcorredorro      3                  53.33      4   \n",
       "161         161          feartheGru      3                 358.33      4   \n",
       "162         162        Danielsv9207      3                 358.33      4   \n",
       "163         163  JhonEmmanuelTorres      3                 358.33      4   \n",
       "\n",
       "     Q7_Q7_1  Q7_Q7_2  Q8_Q8_1  Q10  similarity  \n",
       "0        0.0      0.0      4.0  1.0    0.146341  \n",
       "1        4.0      4.0      5.0  3.0    0.057325  \n",
       "2        0.0      2.0      3.0  1.0    0.190476  \n",
       "3        4.0      3.0      5.0  2.0    0.003841  \n",
       "4        2.0      6.0      6.0  2.0    0.004854  \n",
       "..       ...      ...      ...  ...         ...  \n",
       "159      3.0      4.0      3.0  2.0    0.009360  \n",
       "160      1.0      1.0      5.0  2.0    0.013294  \n",
       "161      2.0      2.0      5.0  2.0    0.000000  \n",
       "162      1.0      1.0      1.0  3.0    0.000000  \n",
       "163      5.0      4.0      5.0  2.0    0.000000  \n",
       "\n",
       "[160 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('perform_dist_high_compare_b3.csv', header=0)\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b7a992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'full' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "full = df\n",
    "%store full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d9ca75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              OLS Regression Results                             \n",
      "=================================================================================\n",
      "Dep. Variable:     abs_perform_diff_best   R-squared:                       0.086\n",
      "Model:                               OLS   Adj. R-squared:                  0.080\n",
      "Method:                    Least Squares   F-statistic:                     14.87\n",
      "Date:                   Sat, 24 Sep 2022   Prob (F-statistic):           0.000167\n",
      "Time:                           23:26:45   Log-Likelihood:                -913.78\n",
      "No. Observations:                    160   AIC:                             1832.\n",
      "Df Residuals:                        158   BIC:                             1838.\n",
      "Df Model:                              1                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    258.9941     15.856     16.334      0.000     227.678     290.311\n",
      "Q10          -30.3360      7.867     -3.856      0.000     -45.873     -14.799\n",
      "==============================================================================\n",
      "Omnibus:                        0.652   Durbin-Watson:                   1.534\n",
      "Prob(Omnibus):                  0.722   Jarque-Bera (JB):                0.319\n",
      "Skew:                          -0.019   Prob(JB):                        0.853\n",
      "Kurtosis:                       3.215   Cond. No.                         6.70\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                          IV2SLS Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          ln_similarity   R-squared:                       0.045\n",
      "Model:                         IV2SLS   Adj. R-squared:                  0.039\n",
      "Method:                     Two Stage   F-statistic:                    0.1466\n",
      "                        Least Squares   Prob (F-statistic):              0.702\n",
      "Date:                Sat, 24 Sep 2022                                         \n",
      "Time:                        23:26:45                                         \n",
      "No. Observations:                 160                                         \n",
      "Df Residuals:                     158                                         \n",
      "Df Model:                           1                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "const                     0.0735      0.039      1.908      0.058      -0.003       0.150\n",
      "abs_perform_diff_best -7.251e-05      0.000     -0.383      0.702      -0.000       0.000\n",
      "==============================================================================\n",
      "Omnibus:                       13.088   Durbin-Watson:                   1.207\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               14.128\n",
      "Skew:                           0.698   Prob(JB):                     0.000855\n",
      "Kurtosis:                       2.589   Cond. No.                         611.\n",
      "==============================================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          ln_similarity   R-squared:                       0.075\n",
      "Model:                            OLS   Adj. R-squared:                  0.069\n",
      "Method:                 Least Squares   F-statistic:                     12.81\n",
      "Date:                Sat, 24 Sep 2022   Prob (F-statistic):           0.000457\n",
      "Time:                        23:26:45   Log-Likelihood:                 244.29\n",
      "No. Observations:                 160   AIC:                            -484.6\n",
      "Df Residuals:                     158   BIC:                            -478.4\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                 0.0984      0.012      8.327      0.000       0.075       0.122\n",
      "abs_perform_diff_best    -0.0002   5.47e-05     -3.580      0.000      -0.000   -8.77e-05\n",
      "==============================================================================\n",
      "Omnibus:                       10.169   Durbin-Watson:                   1.191\n",
      "Prob(Omnibus):                  0.006   Jarque-Bera (JB):               10.965\n",
      "Skew:                           0.626   Prob(JB):                      0.00416\n",
      "Kurtosis:                       2.725   Cond. No.                         611.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Let's confirm that aspiration satisfy the relevance condition for performance distance to the best. There is a difference between aspiration and social aspiration. \n",
    "reg_expr = 'abs_perform_diff_best ~ Q10'\n",
    "\n",
    "# Build and train an OLS model that regresses performance distance to the best on aspiration and verify\n",
    "# using the F-test that coefficients of aspiration is significant \n",
    "olsr_model = smf.ols(formula=reg_expr, data=df)\n",
    "olsr_model_results = olsr_model.fit()\n",
    "print(olsr_model_results.summary())\n",
    "\n",
    "df['ln_similarity'] = np.log(df['similarity'] + 1)\n",
    "\n",
    "# Build out the exog matrix. Statsmodels requires this matrix to contain all the endogenous and\n",
    "# exogenous variables, plus the constant.\n",
    "exog = df[['abs_perform_diff_best']]\n",
    "exog = add_constant(exog)\n",
    "\n",
    "# Build out the instruments matrix. Statsmodels requires this matrix to contain not only all the\n",
    "# instruments but also the variables in exog that will NOT be instrumented\n",
    "instruments = df[['Q10']]\n",
    "instruments = add_constant(instruments)\n",
    "\n",
    "#Build and train the IV2SLS model\n",
    "iv2sls_model = IV2SLS(endog=df['ln_similarity'], exog=exog, instrument=instruments)\n",
    "iv2sls_model_results = iv2sls_model.fit()\n",
    "\n",
    "#Print the training summary\n",
    "print(iv2sls_model_results.summary())\n",
    "\n",
    "#Compare the performance of 2SLS with OLS of ln(wage) on performance distance to the best\n",
    "reg_expr = 'ln_similarity ~ abs_perform_diff_best'\n",
    "olsr_model = smf.ols(formula=reg_expr, data=df)\n",
    "olsr_model_results = olsr_model.fit()\n",
    "print(olsr_model_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc682f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "711ff7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Student</th>\n",
       "      <th>group</th>\n",
       "      <th>abs_perform_diff_best</th>\n",
       "      <th>phase</th>\n",
       "      <th>Q7_Q7_1</th>\n",
       "      <th>Q7_Q7_2</th>\n",
       "      <th>Q8_Q8_1</th>\n",
       "      <th>Q10</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>lemartinp</td>\n",
       "      <td>3</td>\n",
       "      <td>193.33</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>juligarji</td>\n",
       "      <td>3</td>\n",
       "      <td>126.67</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.009495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>nfmorenog</td>\n",
       "      <td>3</td>\n",
       "      <td>196.67</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.139535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>AFelipeGA</td>\n",
       "      <td>3</td>\n",
       "      <td>177.50</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.010793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>mballeng91</td>\n",
       "      <td>3</td>\n",
       "      <td>230.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.007353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>dduartec</td>\n",
       "      <td>3</td>\n",
       "      <td>25.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.092896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>srmedinac</td>\n",
       "      <td>3</td>\n",
       "      <td>214.17</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.055944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>wilson911013</td>\n",
       "      <td>3</td>\n",
       "      <td>239.17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>wapiravaguens</td>\n",
       "      <td>3</td>\n",
       "      <td>137.50</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.071856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>davidh17</td>\n",
       "      <td>3</td>\n",
       "      <td>109.17</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.164557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>DianaNavarrete</td>\n",
       "      <td>3</td>\n",
       "      <td>182.50</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>jdmonroyg</td>\n",
       "      <td>3</td>\n",
       "      <td>330.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>jorge52an</td>\n",
       "      <td>3</td>\n",
       "      <td>167.50</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.044077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>jscastelblancoh</td>\n",
       "      <td>3</td>\n",
       "      <td>99.17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.083665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>juclopezso</td>\n",
       "      <td>3</td>\n",
       "      <td>218.33</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.059829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>afforeroc</td>\n",
       "      <td>3</td>\n",
       "      <td>226.67</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.051546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>dagofonseca</td>\n",
       "      <td>3</td>\n",
       "      <td>102.50</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.079755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>lacardenasv</td>\n",
       "      <td>3</td>\n",
       "      <td>65.83</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.013270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>oacastillol</td>\n",
       "      <td>3</td>\n",
       "      <td>209.17</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>101</td>\n",
       "      <td>ncampuzano</td>\n",
       "      <td>3</td>\n",
       "      <td>213.33</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.064706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>102</td>\n",
       "      <td>adriel62</td>\n",
       "      <td>3</td>\n",
       "      <td>139.17</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>103</td>\n",
       "      <td>capinzor</td>\n",
       "      <td>3</td>\n",
       "      <td>122.50</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.013535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>104</td>\n",
       "      <td>ofnanezn</td>\n",
       "      <td>3</td>\n",
       "      <td>181.67</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.055172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>105</td>\n",
       "      <td>macuestap</td>\n",
       "      <td>3</td>\n",
       "      <td>185.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>spenas</td>\n",
       "      <td>3</td>\n",
       "      <td>214.17</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.026316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>107</td>\n",
       "      <td>kgbayala528</td>\n",
       "      <td>3</td>\n",
       "      <td>78.33</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.109290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>lgmoralesa</td>\n",
       "      <td>3</td>\n",
       "      <td>20.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.134228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>109</td>\n",
       "      <td>ronaldsg20</td>\n",
       "      <td>3</td>\n",
       "      <td>172.50</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.091892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>110</td>\n",
       "      <td>cccristanchoc</td>\n",
       "      <td>3</td>\n",
       "      <td>226.67</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.110390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>111</td>\n",
       "      <td>jmalvarezd</td>\n",
       "      <td>3</td>\n",
       "      <td>55.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.137405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>112</td>\n",
       "      <td>smarquezo</td>\n",
       "      <td>3</td>\n",
       "      <td>218.33</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>113</td>\n",
       "      <td>sarizag</td>\n",
       "      <td>3</td>\n",
       "      <td>201.67</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.095652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>114</td>\n",
       "      <td>Chranium</td>\n",
       "      <td>3</td>\n",
       "      <td>235.00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.030928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>DavidQP</td>\n",
       "      <td>3</td>\n",
       "      <td>205.83</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.053030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>116</td>\n",
       "      <td>afceballosr</td>\n",
       "      <td>3</td>\n",
       "      <td>226.67</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.135593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>117</td>\n",
       "      <td>lizzyt10h</td>\n",
       "      <td>3</td>\n",
       "      <td>184.17</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>118</td>\n",
       "      <td>NicolasPrr</td>\n",
       "      <td>3</td>\n",
       "      <td>221.67</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.048847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>120</td>\n",
       "      <td>feartheGru</td>\n",
       "      <td>3</td>\n",
       "      <td>330.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>121</td>\n",
       "      <td>Danielsv9207</td>\n",
       "      <td>3</td>\n",
       "      <td>330.00</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>JhonEmmanuelTorres</td>\n",
       "      <td>3</td>\n",
       "      <td>330.00</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0             Student  group  abs_perform_diff_best  phase  \\\n",
       "82           82           lemartinp      3                 193.33      3   \n",
       "83           83           juligarji      3                 126.67      3   \n",
       "84           84           nfmorenog      3                 196.67      3   \n",
       "85           85           AFelipeGA      3                 177.50      3   \n",
       "86           86          mballeng91      3                 230.00      3   \n",
       "87           87            dduartec      3                  25.00      3   \n",
       "88           88           srmedinac      3                 214.17      3   \n",
       "89           89        wilson911013      3                 239.17      3   \n",
       "90           90       wapiravaguens      3                 137.50      3   \n",
       "91           91            davidh17      3                 109.17      3   \n",
       "92           92      DianaNavarrete      3                 182.50      3   \n",
       "93           93           jdmonroyg      3                 330.00      3   \n",
       "94           94           jorge52an      3                 167.50      3   \n",
       "95           95     jscastelblancoh      3                  99.17      3   \n",
       "96           96          juclopezso      3                 218.33      3   \n",
       "97           97           afforeroc      3                 226.67      3   \n",
       "98           98         dagofonseca      3                 102.50      3   \n",
       "99           99         lacardenasv      3                  65.83      3   \n",
       "100         100         oacastillol      3                 209.17      3   \n",
       "101         101          ncampuzano      3                 213.33      3   \n",
       "102         102            adriel62      3                 139.17      3   \n",
       "103         103            capinzor      3                 122.50      3   \n",
       "104         104            ofnanezn      3                 181.67      3   \n",
       "105         105           macuestap      3                 185.00      3   \n",
       "106         106              spenas      3                 214.17      3   \n",
       "107         107         kgbayala528      3                  78.33      3   \n",
       "108         108          lgmoralesa      3                  20.00      3   \n",
       "109         109          ronaldsg20      3                 172.50      3   \n",
       "110         110       cccristanchoc      3                 226.67      3   \n",
       "111         111          jmalvarezd      3                  55.00      3   \n",
       "112         112           smarquezo      3                 218.33      3   \n",
       "113         113             sarizag      3                 201.67      3   \n",
       "114         114            Chranium      3                 235.00      3   \n",
       "115         115             DavidQP      3                 205.83      3   \n",
       "116         116         afceballosr      3                 226.67      3   \n",
       "117         117           lizzyt10h      3                 184.17      3   \n",
       "118         118          NicolasPrr      3                 221.67      3   \n",
       "120         120          feartheGru      3                 330.00      3   \n",
       "121         121        Danielsv9207      3                 330.00      3   \n",
       "122         122  JhonEmmanuelTorres      3                 330.00      3   \n",
       "\n",
       "     Q7_Q7_1  Q7_Q7_2  Q8_Q8_1  Q10  similarity  \n",
       "82       0.0      0.0      4.0  1.0    0.100000  \n",
       "83       4.0      4.0      5.0  3.0    0.009495  \n",
       "84       0.0      2.0      3.0  1.0    0.139535  \n",
       "85       4.0      3.0      5.0  2.0    0.010793  \n",
       "86       2.0      6.0      6.0  2.0    0.007353  \n",
       "87       1.0      5.0      4.0  1.0    0.092896  \n",
       "88       1.0      2.0      4.0  1.0    0.055944  \n",
       "89       0.0      1.0      2.0  1.0    0.010309  \n",
       "90       0.0      1.0      4.0  2.0    0.071856  \n",
       "91       3.0      3.0      5.0  3.0    0.164557  \n",
       "92       0.0      1.0      5.0  2.0    0.030303  \n",
       "93       0.0      1.0      4.0  1.0    0.000000  \n",
       "94       4.0      4.0      5.0  2.0    0.044077  \n",
       "95       0.0      0.0      3.0  2.0    0.083665  \n",
       "96       1.0      3.0      3.0  2.0    0.059829  \n",
       "97       1.0      4.0      3.0  2.0    0.051546  \n",
       "98       0.0      0.0      2.0  2.0    0.079755  \n",
       "99       1.0      2.0      5.0  4.0    0.013270  \n",
       "100      2.0      1.0      5.0  1.0    0.075718  \n",
       "101      1.0      3.0      4.0  2.0    0.064706  \n",
       "102      1.0      3.0      5.0  2.0    0.005741  \n",
       "103      0.0      0.0      2.0  2.0    0.013535  \n",
       "104      0.0      1.0      6.0  2.0    0.055172  \n",
       "105      0.0      1.0      4.0  1.0    0.076923  \n",
       "106      5.0      4.0      6.0  1.0    0.026316  \n",
       "107      1.0      5.0      6.0  2.0    0.109290  \n",
       "108      2.0      2.0      4.0  3.0    0.134228  \n",
       "109      0.0      3.0      5.0  2.0    0.091892  \n",
       "110      1.0      1.0      4.0  1.0    0.110390  \n",
       "111      3.0      3.0      5.0  2.0    0.137405  \n",
       "112      0.0      0.0      2.0  0.0    0.054545  \n",
       "113      4.0      3.0      6.0  2.0    0.095652  \n",
       "114      4.0      5.0      6.0  2.0    0.030928  \n",
       "115      2.0      3.0      4.0  3.0    0.053030  \n",
       "116      1.0      3.0      5.0  2.0    0.135593  \n",
       "117      2.0      1.0      5.0  2.0    0.095238  \n",
       "118      3.0      4.0      3.0  2.0    0.048847  \n",
       "120      2.0      2.0      5.0  2.0    0.000000  \n",
       "121      1.0      1.0      1.0  3.0    0.000000  \n",
       "122      5.0      4.0      5.0  2.0    0.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('perform_dist_high_compare_b3.csv', header=0)\n",
    "df = df.dropna()\n",
    "df = df[df['phase'] == 3]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21101318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              OLS Regression Results                             \n",
      "=================================================================================\n",
      "Dep. Variable:     abs_perform_diff_best   R-squared:                       0.076\n",
      "Model:                               OLS   Adj. R-squared:                  0.052\n",
      "Method:                    Least Squares   F-statistic:                     3.138\n",
      "Date:                   Sat, 24 Sep 2022   Prob (F-statistic):             0.0845\n",
      "Time:                           23:26:45   Log-Likelihood:                -228.46\n",
      "No. Observations:                     40   AIC:                             460.9\n",
      "Df Residuals:                         38   BIC:                             464.3\n",
      "Df Model:                              1                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    236.2613     32.027      7.377      0.000     171.427     301.096\n",
      "Q10          -28.1056     15.865     -1.772      0.084     -60.223       4.012\n",
      "==============================================================================\n",
      "Omnibus:                        1.718   Durbin-Watson:                   1.232\n",
      "Prob(Omnibus):                  0.424   Jarque-Bera (JB):                0.790\n",
      "Skew:                           0.125   Prob(JB):                        0.674\n",
      "Kurtosis:                       3.641   Cond. No.                         6.63\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                          IV2SLS Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          ln_similarity   R-squared:                      -0.383\n",
      "Model:                         IV2SLS   Adj. R-squared:                 -0.419\n",
      "Method:                     Two Stage   F-statistic:                    0.1909\n",
      "                        Least Squares   Prob (F-statistic):              0.665\n",
      "Date:                Sat, 24 Sep 2022                                         \n",
      "Time:                        23:26:45                                         \n",
      "No. Observations:                  40                                         \n",
      "Df Residuals:                      38                                         \n",
      "Df Model:                           1                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "const                     0.0273      0.071      0.383      0.704      -0.117       0.172\n",
      "abs_perform_diff_best     0.0002      0.000      0.437      0.665      -0.001       0.001\n",
      "==============================================================================\n",
      "Omnibus:                        1.053   Durbin-Watson:                   1.567\n",
      "Prob(Omnibus):                  0.591   Jarque-Bera (JB):                0.992\n",
      "Skew:                           0.206   Prob(JB):                        0.609\n",
      "Kurtosis:                       2.348   Cond. No.                         519.\n",
      "==============================================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          ln_similarity   R-squared:                       0.233\n",
      "Model:                            OLS   Adj. R-squared:                  0.213\n",
      "Method:                 Least Squares   F-statistic:                     11.57\n",
      "Date:                Sat, 24 Sep 2022   Prob (F-statistic):            0.00159\n",
      "Time:                        23:26:45   Log-Likelihood:                 74.831\n",
      "No. Observations:                  40   AIC:                            -145.7\n",
      "Df Residuals:                      38   BIC:                            -142.3\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                 0.1079      0.016      6.836      0.000       0.076       0.140\n",
      "abs_perform_diff_best    -0.0003   7.94e-05     -3.401      0.002      -0.000      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                        0.225   Durbin-Watson:                   2.003\n",
      "Prob(Omnibus):                  0.894   Jarque-Bera (JB):                0.181\n",
      "Skew:                           0.145   Prob(JB):                        0.914\n",
      "Kurtosis:                       2.845   Cond. No.                         519.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Let's confirm that aspiration satisfy the relevance condition for performance distance to the best. There is a difference between aspiration and social aspiration. \n",
    "reg_expr = 'abs_perform_diff_best ~ Q10'\n",
    "\n",
    "# Build and train an OLS model that regresses performance distance to the best on aspiration and verify\n",
    "# using the F-test that coefficients of aspiration is significant \n",
    "olsr_model = smf.ols(formula=reg_expr, data=df)\n",
    "olsr_model_results = olsr_model.fit()\n",
    "print(olsr_model_results.summary())\n",
    "\n",
    "df['ln_similarity'] = np.log(df['similarity'] + 1)\n",
    "\n",
    "# Build out the exog matrix. Statsmodels requires this matrix to contain all the endogenous and\n",
    "# exogenous variables, plus the constant.\n",
    "exog = df[['abs_perform_diff_best']]\n",
    "exog = add_constant(exog)\n",
    "\n",
    "# Build out the instruments matrix. Statsmodels requires this matrix to contain not only all the\n",
    "# instruments but also the variables in exog that will NOT be instrumented\n",
    "instruments = df[['Q10']]\n",
    "instruments = add_constant(instruments)\n",
    "\n",
    "#Build and train the IV2SLS model\n",
    "iv2sls_model = IV2SLS(endog=df['ln_similarity'], exog=exog, instrument=instruments)\n",
    "iv2sls_model_results = iv2sls_model.fit()\n",
    "\n",
    "#Print the training summary\n",
    "print(iv2sls_model_results.summary())\n",
    "\n",
    "#Compare the performance of 2SLS with OLS of ln(wage) on performance distance to the best\n",
    "reg_expr = 'ln_similarity ~ abs_perform_diff_best'\n",
    "olsr_model = smf.ols(formula=reg_expr, data=df)\n",
    "olsr_model_results = olsr_model.fit()\n",
    "print(olsr_model_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38812be3",
   "metadata": {},
   "source": [
    "## Phase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf82f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Student</th>\n",
       "      <th>group</th>\n",
       "      <th>abs_perform_diff_best</th>\n",
       "      <th>phase</th>\n",
       "      <th>Q7_Q7_1</th>\n",
       "      <th>Q7_Q7_2</th>\n",
       "      <th>Q8_Q8_1</th>\n",
       "      <th>Q10</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>lemartinp</td>\n",
       "      <td>3</td>\n",
       "      <td>57.50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>125</td>\n",
       "      <td>nfmorenog</td>\n",
       "      <td>3</td>\n",
       "      <td>237.50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>AFelipeGA</td>\n",
       "      <td>3</td>\n",
       "      <td>205.83</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.028254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>127</td>\n",
       "      <td>mballeng91</td>\n",
       "      <td>3</td>\n",
       "      <td>245.83</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>128</td>\n",
       "      <td>dduartec</td>\n",
       "      <td>3</td>\n",
       "      <td>270.83</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.030488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>129</td>\n",
       "      <td>srmedinac</td>\n",
       "      <td>3</td>\n",
       "      <td>242.50</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>130</td>\n",
       "      <td>wilson911013</td>\n",
       "      <td>3</td>\n",
       "      <td>275.83</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>131</td>\n",
       "      <td>wapiravaguens</td>\n",
       "      <td>3</td>\n",
       "      <td>140.83</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>132</td>\n",
       "      <td>davidh17</td>\n",
       "      <td>3</td>\n",
       "      <td>283.33</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.007602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>133</td>\n",
       "      <td>DianaNavarrete</td>\n",
       "      <td>3</td>\n",
       "      <td>242.50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.001559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>134</td>\n",
       "      <td>jdmonroyg</td>\n",
       "      <td>3</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>jorge52an</td>\n",
       "      <td>3</td>\n",
       "      <td>115.00</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.014080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>136</td>\n",
       "      <td>jscastelblancoh</td>\n",
       "      <td>3</td>\n",
       "      <td>41.66</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.013668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>137</td>\n",
       "      <td>juclopezso</td>\n",
       "      <td>3</td>\n",
       "      <td>206.66</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.003871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>138</td>\n",
       "      <td>afforeroc</td>\n",
       "      <td>3</td>\n",
       "      <td>259.16</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.001949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>139</td>\n",
       "      <td>dagofonseca</td>\n",
       "      <td>3</td>\n",
       "      <td>93.33</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.013228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>140</td>\n",
       "      <td>lacardenasv</td>\n",
       "      <td>3</td>\n",
       "      <td>98.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.028501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>141</td>\n",
       "      <td>oacastillol</td>\n",
       "      <td>3</td>\n",
       "      <td>237.50</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>142</td>\n",
       "      <td>ncampuzano</td>\n",
       "      <td>3</td>\n",
       "      <td>213.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.016510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>143</td>\n",
       "      <td>adriel62</td>\n",
       "      <td>3</td>\n",
       "      <td>147.50</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.017557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>144</td>\n",
       "      <td>capinzor</td>\n",
       "      <td>3</td>\n",
       "      <td>163.33</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.029659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>145</td>\n",
       "      <td>ofnanezn</td>\n",
       "      <td>3</td>\n",
       "      <td>230.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>146</td>\n",
       "      <td>macuestap</td>\n",
       "      <td>3</td>\n",
       "      <td>209.16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>spenas</td>\n",
       "      <td>3</td>\n",
       "      <td>242.50</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>148</td>\n",
       "      <td>kgbayala528</td>\n",
       "      <td>3</td>\n",
       "      <td>115.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.007925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>149</td>\n",
       "      <td>lgmoralesa</td>\n",
       "      <td>3</td>\n",
       "      <td>20.83</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.004566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>150</td>\n",
       "      <td>ronaldsg20</td>\n",
       "      <td>3</td>\n",
       "      <td>192.50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.008037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>151</td>\n",
       "      <td>cccristanchoc</td>\n",
       "      <td>3</td>\n",
       "      <td>242.50</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>152</td>\n",
       "      <td>jmalvarezd</td>\n",
       "      <td>3</td>\n",
       "      <td>140.00</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>153</td>\n",
       "      <td>smarquezo</td>\n",
       "      <td>3</td>\n",
       "      <td>246.66</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>154</td>\n",
       "      <td>sarizag</td>\n",
       "      <td>3</td>\n",
       "      <td>167.50</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155</td>\n",
       "      <td>Chranium</td>\n",
       "      <td>3</td>\n",
       "      <td>263.33</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.001170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156</td>\n",
       "      <td>DavidQP</td>\n",
       "      <td>3</td>\n",
       "      <td>86.66</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.007692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>157</td>\n",
       "      <td>afceballosr</td>\n",
       "      <td>3</td>\n",
       "      <td>93.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.004241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>158</td>\n",
       "      <td>lizzyt10h</td>\n",
       "      <td>3</td>\n",
       "      <td>212.50</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.008024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>NicolasPrr</td>\n",
       "      <td>3</td>\n",
       "      <td>237.50</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.009360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>jumcorredorro</td>\n",
       "      <td>3</td>\n",
       "      <td>53.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.013294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>feartheGru</td>\n",
       "      <td>3</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>Danielsv9207</td>\n",
       "      <td>3</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>JhonEmmanuelTorres</td>\n",
       "      <td>3</td>\n",
       "      <td>358.33</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0             Student  group  abs_perform_diff_best  phase  \\\n",
       "123         123           lemartinp      3                  57.50      4   \n",
       "125         125           nfmorenog      3                 237.50      4   \n",
       "126         126           AFelipeGA      3                 205.83      4   \n",
       "127         127          mballeng91      3                 245.83      4   \n",
       "128         128            dduartec      3                 270.83      4   \n",
       "129         129           srmedinac      3                 242.50      4   \n",
       "130         130        wilson911013      3                 275.83      4   \n",
       "131         131       wapiravaguens      3                 140.83      4   \n",
       "132         132            davidh17      3                 283.33      4   \n",
       "133         133      DianaNavarrete      3                 242.50      4   \n",
       "134         134           jdmonroyg      3                 358.33      4   \n",
       "135         135           jorge52an      3                 115.00      4   \n",
       "136         136     jscastelblancoh      3                  41.66      4   \n",
       "137         137          juclopezso      3                 206.66      4   \n",
       "138         138           afforeroc      3                 259.16      4   \n",
       "139         139         dagofonseca      3                  93.33      4   \n",
       "140         140         lacardenasv      3                  98.33      4   \n",
       "141         141         oacastillol      3                 237.50      4   \n",
       "142         142          ncampuzano      3                 213.33      4   \n",
       "143         143            adriel62      3                 147.50      4   \n",
       "144         144            capinzor      3                 163.33      4   \n",
       "145         145            ofnanezn      3                 230.00      4   \n",
       "146         146           macuestap      3                 209.16      4   \n",
       "147         147              spenas      3                 242.50      4   \n",
       "148         148         kgbayala528      3                 115.00      4   \n",
       "149         149          lgmoralesa      3                  20.83      4   \n",
       "150         150          ronaldsg20      3                 192.50      4   \n",
       "151         151       cccristanchoc      3                 242.50      4   \n",
       "152         152          jmalvarezd      3                 140.00      4   \n",
       "153         153           smarquezo      3                 246.66      4   \n",
       "154         154             sarizag      3                 167.50      4   \n",
       "155         155            Chranium      3                 263.33      4   \n",
       "156         156             DavidQP      3                  86.66      4   \n",
       "157         157         afceballosr      3                  93.33      4   \n",
       "158         158           lizzyt10h      3                 212.50      4   \n",
       "159         159          NicolasPrr      3                 237.50      4   \n",
       "160         160       jumcorredorro      3                  53.33      4   \n",
       "161         161          feartheGru      3                 358.33      4   \n",
       "162         162        Danielsv9207      3                 358.33      4   \n",
       "163         163  JhonEmmanuelTorres      3                 358.33      4   \n",
       "\n",
       "     Q7_Q7_1  Q7_Q7_2  Q8_Q8_1  Q10  similarity  \n",
       "123      0.0      0.0      4.0  1.0    0.005437  \n",
       "125      0.0      2.0      3.0  1.0    0.006931  \n",
       "126      4.0      3.0      5.0  2.0    0.028254  \n",
       "127      2.0      6.0      6.0  2.0    0.005132  \n",
       "128      1.0      5.0      4.0  1.0    0.030488  \n",
       "129      1.0      2.0      4.0  1.0    0.005760  \n",
       "130      0.0      1.0      2.0  1.0    0.000390  \n",
       "131      0.0      1.0      4.0  2.0    0.006834  \n",
       "132      3.0      3.0      5.0  3.0    0.007602  \n",
       "133      0.0      1.0      5.0  2.0    0.001559  \n",
       "134      0.0      1.0      4.0  1.0    0.004287  \n",
       "135      4.0      4.0      5.0  2.0    0.014080  \n",
       "136      0.0      0.0      3.0  2.0    0.013668  \n",
       "137      1.0      3.0      3.0  2.0    0.003871  \n",
       "138      1.0      4.0      3.0  2.0    0.001949  \n",
       "139      0.0      0.0      2.0  2.0    0.013228  \n",
       "140      1.0      2.0      5.0  4.0    0.028501  \n",
       "141      2.0      1.0      5.0  1.0    0.016596  \n",
       "142      1.0      3.0      4.0  2.0    0.016510  \n",
       "143      1.0      3.0      5.0  2.0    0.017557  \n",
       "144      0.0      0.0      2.0  2.0    0.029659  \n",
       "145      0.0      1.0      6.0  2.0    0.005756  \n",
       "146      0.0      1.0      4.0  1.0    0.003110  \n",
       "147      5.0      4.0      6.0  1.0    0.001550  \n",
       "148      1.0      5.0      6.0  2.0    0.007925  \n",
       "149      2.0      2.0      4.0  3.0    0.004566  \n",
       "150      0.0      3.0      5.0  2.0    0.008037  \n",
       "151      1.0      1.0      4.0  1.0    0.008330  \n",
       "152      3.0      3.0      5.0  2.0    0.005380  \n",
       "153      0.0      0.0      2.0  0.0    0.004589  \n",
       "154      4.0      3.0      6.0  2.0    0.005019  \n",
       "155      4.0      5.0      6.0  2.0    0.001170  \n",
       "156      2.0      3.0      4.0  3.0    0.007692  \n",
       "157      1.0      3.0      5.0  2.0    0.004241  \n",
       "158      2.0      1.0      5.0  2.0    0.008024  \n",
       "159      3.0      4.0      3.0  2.0    0.009360  \n",
       "160      1.0      1.0      5.0  2.0    0.013294  \n",
       "161      2.0      2.0      5.0  2.0    0.000000  \n",
       "162      1.0      1.0      1.0  3.0    0.000000  \n",
       "163      5.0      4.0      5.0  2.0    0.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('perform_dist_high_compare_b3.csv', header=0)\n",
    "df = df.dropna()\n",
    "df = df[df['phase'] == 4]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6035d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              OLS Regression Results                             \n",
      "=================================================================================\n",
      "Dep. Variable:     abs_perform_diff_best   R-squared:                       0.080\n",
      "Model:                               OLS   Adj. R-squared:                  0.055\n",
      "Method:                    Least Squares   F-statistic:                     3.290\n",
      "Date:                   Sat, 24 Sep 2022   Prob (F-statistic):             0.0776\n",
      "Time:                           23:26:45   Log-Likelihood:                -234.48\n",
      "No. Observations:                     40   AIC:                             473.0\n",
      "Df Residuals:                         38   BIC:                             476.3\n",
      "Df Model:                              1                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    262.8576     37.748      6.964      0.000     186.441     339.274\n",
      "Q10          -34.4514     18.993     -1.814      0.078     -72.901       3.998\n",
      "==============================================================================\n",
      "Omnibus:                        0.353   Durbin-Watson:                   1.480\n",
      "Prob(Omnibus):                  0.838   Jarque-Bera (JB):                0.204\n",
      "Skew:                           0.170   Prob(JB):                        0.903\n",
      "Kurtosis:                       2.915   Cond. No.                         6.67\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                          IV2SLS Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          ln_similarity   R-squared:                       0.033\n",
      "Model:                         IV2SLS   Adj. R-squared:                  0.007\n",
      "Method:                     Two Stage   F-statistic:                     1.012\n",
      "                        Least Squares   Prob (F-statistic):              0.321\n",
      "Date:                Sat, 24 Sep 2022                                         \n",
      "Time:                        23:26:45                                         \n",
      "No. Observations:                  40                                         \n",
      "Df Residuals:                      38                                         \n",
      "Df Model:                           1                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "const                     0.0192      0.010      1.853      0.072      -0.002       0.040\n",
      "abs_perform_diff_best -5.183e-05   5.15e-05     -1.006      0.321      -0.000    5.25e-05\n",
      "==============================================================================\n",
      "Omnibus:                       15.556   Durbin-Watson:                   1.525\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               17.454\n",
      "Skew:                           1.346   Prob(JB):                     0.000162\n",
      "Kurtosis:                       4.796   Cond. No.                         536.\n",
      "==============================================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          ln_similarity   R-squared:                       0.098\n",
      "Model:                            OLS   Adj. R-squared:                  0.074\n",
      "Method:                 Least Squares   F-statistic:                     4.132\n",
      "Date:                Sat, 24 Sep 2022   Prob (F-statistic):             0.0491\n",
      "Time:                        23:26:45   Log-Likelihood:                 138.04\n",
      "No. Observations:                  40   AIC:                            -272.1\n",
      "Df Residuals:                      38   BIC:                            -268.7\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "Intercept                 0.0145      0.003      4.744      0.000       0.008       0.021\n",
      "abs_perform_diff_best -2.855e-05    1.4e-05     -2.033      0.049    -5.7e-05   -1.18e-07\n",
      "==============================================================================\n",
      "Omnibus:                       18.997   Durbin-Watson:                   1.521\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               23.319\n",
      "Skew:                           1.615   Prob(JB):                     8.64e-06\n",
      "Kurtosis:                       4.888   Cond. No.                         536.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Let's confirm that aspiration satisfy the relevance condition for performance distance to the best. There is a difference between aspiration and social aspiration. \n",
    "reg_expr = 'abs_perform_diff_best ~ Q10'\n",
    "\n",
    "# Build and train an OLS model that regresses performance distance to the best on aspiration and verify\n",
    "# using the F-test that coefficients of aspiration is significant \n",
    "olsr_model = smf.ols(formula=reg_expr, data=df)\n",
    "olsr_model_results = olsr_model.fit()\n",
    "print(olsr_model_results.summary())\n",
    "\n",
    "df['ln_similarity'] = np.log(df['similarity'] + 1)\n",
    "\n",
    "# Build out the exog matrix. Statsmodels requires this matrix to contain all the endogenous and\n",
    "# exogenous variables, plus the constant.\n",
    "exog = df[['abs_perform_diff_best']]\n",
    "exog = add_constant(exog)\n",
    "\n",
    "# Build out the instruments matrix. Statsmodels requires this matrix to contain not only all the\n",
    "# instruments but also the variables in exog that will NOT be instrumented\n",
    "instruments = df[['Q10']]\n",
    "instruments = add_constant(instruments)\n",
    "\n",
    "#Build and train the IV2SLS model\n",
    "iv2sls_model = IV2SLS(endog=df['ln_similarity'], exog=exog, instrument=instruments)\n",
    "iv2sls_model_results = iv2sls_model.fit()\n",
    "\n",
    "#Print the training summary\n",
    "print(iv2sls_model_results.summary())\n",
    "\n",
    "#Compare the performance of 2SLS with OLS of ln(wage) on performance distance to the best\n",
    "reg_expr = 'ln_similarity ~ abs_perform_diff_best'\n",
    "olsr_model = smf.ols(formula=reg_expr, data=df)\n",
    "olsr_model_results = olsr_model.fit()\n",
    "print(olsr_model_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816cff2",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0da5ae",
   "metadata": {},
   "source": [
    "![image1a](image1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf987c",
   "metadata": {},
   "source": [
    "The coefficients of aspiration (Q10) is significant at a p of < 0 as indicated by their p-values which are basically zero. Aspiration (Q10) clearly meet the **relevance condition** for instrumental variables of performance distance. \n",
    "\n",
    "Weâ€™ll now build a linear model for the similarity to the best equation and using statsmodels, weâ€™ll train the model using the two-stage least square estimator.\n",
    "\n",
    "Weâ€™ll start by building the design matrices. The dependent variable is ln(similarity). \n",
    "\n",
    "```\n",
    "ln_wage = np.log(df['similarity'])\n",
    "```\n",
    "\n",
    "Statsmodelâ€™s IV2SLS estimator is defined as follows:\n",
    "\n",
    "```\n",
    "\n",
    "statsmodels.sandbox.regression.gmm.IV2SLS(endog, exog, instrument=None)\n",
    "\n",
    "```\n",
    "Statsmodels needs the endog, exog and instrument matrices to be constructed in a specific way as follows:\n",
    "\n",
    "`endog` is an [n x 1] matrix containing the dependent variable. In our example, it is the log(similarity) variable.\n",
    "\n",
    "`exog` is an [n x (k+1)] size matrix that must contain all the endogenous and exogenous variables, plus the constant. In our example, apart from the constant, we do not have any exogenous variables defined in our wage equation. \n",
    "\n",
    "`instrument` is a matrix that contains the instrumental variables. Additionally, the Statsmodelsâ€™ IV2SLS estimator requires instrument to also contain all variables from the exog matrix that are not being instrumented. In our example, the instrumental variable is aspiration. The variables in exog that are not being instrumented is just the placeholder column for the intercept. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e56287",
   "metadata": {},
   "source": [
    "![image2a](image2a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2aa2cf",
   "metadata": {},
   "source": [
    "## Interpretation of results of the 2SLS model\n",
    "\n",
    "Since our primary interest is in estimating the effect of performance distance to the best on similarity to the best, weâ€™ll focus our attention on the coefficient estimate of the performance distance to the best. \n",
    "\n",
    "We see that the 2SLS model has estimated the coefficient of performance distance to the best as -7.251e-05 with a standard error of 0.000 and a 95% confidence interval of -0.000 to 0.000. The p-value of 0.084 suggests a significance at (1â€“0.702)100%=29.8%. Overall, and as expected for a 2SLS model, the model lacks precision.\n",
    "\n",
    "Note that dependent variable is log(similarity + 1). To calculate the rate of change of similarity to the best for each unit change of performance distance to the best, we must exponentiate the coefficient of performance distance to the best.\n",
    "\n",
    "1 - 10^(-7.251e-05) = 0.000167 implying that a unit decrease in performance distance to the best is estimated to yield an increase of 0.000167 in similarity to the best, and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5794b",
   "metadata": {},
   "source": [
    "## Comparison of the IV estimator with an OLS estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08baa040",
   "metadata": {},
   "source": [
    "Letâ€™s compare the performance of the 2SLS model with a straight-up OLS model that regresses log(similarity) on performance distance to the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf0fc0d",
   "metadata": {},
   "source": [
    "![image3a.png](image3a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f06eb8e",
   "metadata": {},
   "source": [
    "Weâ€™ll focus our attention on the estimated value of the coefficient of performance distance to the best. At -0.0002, it is a lot higher than the estimate reported by the 2SLS model.\n",
    "\n",
    "1 - 10^(-0.0002)=0.00046, implying a unit decrease in the performance distance to the best is estimated to translate into a 0.00046 increase in similarity to the best (vice versa). \n",
    "\n",
    "The higher estimate from OLS is expected due to the suspected endogeniety of performance distance to the best. In practice, depending on the situation we are modeling, we may want to accept the more conservative estimate of -7.251e-05 reported by the 2SLS model. However, (and against the 2SLS model), the coefficient estimate from the OLS model is highly significant with a p-value that is 0.000. Recollect that the estimate from the 2SLS model was significant at only a 29.8% confidence level.\n",
    "\n",
    "The coefficient estimate of performance distance to the best reported by the OLS model has pretty similar standard error which is zero as compared to that from the 2SLS model. \n",
    "\n",
    "For comparison, here are the coefficient estimates of performance distance to the best and corresponding 95% CIs from the two models:\n",
    "\n",
    "With the IV estimator, one trades precision of estimates for the removal of endogeneity and the consequent bias in the estimates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4b679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
